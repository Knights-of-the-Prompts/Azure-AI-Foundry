import os
import time
from pathlib import Path
from urllib.parse import urlparse, parse_qs

from azure.ai.projects import AIProjectClient
from azure.ai.agents.models import ThreadMessage
from azure.storage.blob import BlobServiceClient, ContainerClient

from terminal_colorswh import TerminalColors as tc


class Utilities:
    def log_msg_green(self, msg: str) -> None:
        """Print a message in green."""
        print(f"{tc.GREEN}{msg}{tc.RESET}")

    def log_msg_purple(self, msg: str) -> None:
        """Print a message in purple."""
        print(f"{tc.PURPLE}{msg}{tc.RESET}")

    def log_token_blue(self, msg: str) -> None:
        """Print a token in blue."""
        print(f"{tc.BLUE}{msg}{tc.RESET}", end="", flush=True)

    def get_file(self, project_client: AIProjectClient, file_id: str, attachment_name: str) -> None:
        """Retrieve the file and save it to the local disk."""
        self.log_msg_green(f"Getting file with ID: {file_id}")

        file_name, file_extension = os.path.splitext(
            os.path.basename(attachment_name.split(":")[-1]))
        file_name = f"{file_name}.{file_id}{file_extension}"

        env = os.getenv("ENVIRONMENT", "local")
        folder_path = Path(f"{'src/workshop/' if env == 'container' else ''}files")

        folder_path.mkdir(parents=True, exist_ok=True)

        file_path = folder_path / file_name

        # Save the file using a synchronous context manager
        with file_path.open("wb") as file:
            for chunk in project_client.agents.get_file_content(file_id):
                file.write(chunk)

        self.log_msg_green(f"File saved to {file_path}")
        # Cleanup the remote file
        project_client.agents.delete_file(file_id)

    def get_files(self, message: ThreadMessage, project_client: AIProjectClient) -> None:
        """Get the image files from the message and kickoff download."""
        if message.image_contents:
            for index, image in enumerate(message.image_contents, start=0):
                attachment_name = (
                    "unknown" if not message.file_path_annotations else message.file_path_annotations[
                        index].text
                )
                self.get_file(project_client, image.image_file.file_id, attachment_name)
        elif message.attachments:
            for index, attachment in enumerate(message.attachments, start=0):
                attachment_name = (
                    "unknown" if not message.file_path_annotations else message.file_path_annotations[
                        index].text
                )
                self.get_file(project_client, attachment.file_id, attachment_name)

    def download_agent_files(self, project_client: AIProjectClient, thread_id: str, downloads_dir: str = None) -> None:
        """Download all files generated by the agent (code interpreter, etc.)."""
        try:
            messages = project_client.agents.messages.list(thread_id=thread_id)
            
            # Create downloads directory if it doesn't exist
            import os
            if downloads_dir is None:
                env = os.getenv("ENVIRONMENT", "local")
                downloads_dir = f"{'src/workshop/' if env == 'container' else ''}files"
            
            if not os.path.exists(downloads_dir):
                os.makedirs(downloads_dir)
            
            # Get the latest agent message only (to avoid redownloading old files)
            latest_agent_message = None
            for message in messages:
                if message.role.value == "assistant":
                    latest_agent_message = message
                    break
            
            if not latest_agent_message:
                return  # No agent messages to process
            
            # Track downloaded file IDs to avoid duplicates
            downloaded_file_ids = set()
            
            # First, process file path annotations (primary method for code interpreter files)
            if hasattr(latest_agent_message, 'file_path_annotations') and latest_agent_message.file_path_annotations:
                for file_path_annotation in latest_agent_message.file_path_annotations:
                    file_id = file_path_annotation.file_path.file_id
                    
                    if file_id in downloaded_file_ids:
                        continue  # Skip if already downloaded
                    
                    # Get original filename from the annotation text if possible
                    annotation_text = file_path_annotation.text
                    if "/" in annotation_text:
                        original_filename = annotation_text.split("/")[-1]
                    else:
                        original_filename = f"{file_id}_annotation_file"
                    
                    local_path = os.path.join(downloads_dir, original_filename)
                    
                    try:
                        # Download file content from Azure AI
                        file_content_generator = project_client.agents.files.get_content(file_id=file_id)
                        file_content = b''.join(file_content_generator)
                        with open(local_path, "wb") as f:
                            f.write(file_content)
                        self.log_msg_green(f"Downloaded generated file: {local_path}")
                        downloaded_file_ids.add(file_id)
                    except Exception as e:
                        print(f"Error downloading file {file_id}: {e}")
            
            # Second, check content items for any files not caught by annotations
            if hasattr(latest_agent_message, 'content') and latest_agent_message.content:
                for content_item in latest_agent_message.content:
                    file_id = None
                    file_name = None
                    
                    # Check for different types of content
                    if hasattr(content_item, 'type'):
                        # Handle image_file type
                        if content_item.type == 'image_file' and hasattr(content_item, 'image_file'):
                            file_id = content_item.image_file.file_id
                            file_name = f"{file_id}_image.png"
                        
                        # Handle file_path type (for other files)
                        elif content_item.type == 'file_path' and hasattr(content_item, 'file_path'):
                            file_id = content_item.file_path.file_id
                            file_name = f"{file_id}_file"
                    
                    # Skip if no file found or already downloaded
                    if not file_id or file_id in downloaded_file_ids:
                        continue
                    
                    local_path = os.path.join(downloads_dir, file_name)
                    
                    try:
                        # Download file content from Azure AI
                        file_content_generator = project_client.agents.files.get_content(file_id=file_id)
                        file_content = b''.join(file_content_generator)
                        with open(local_path, "wb") as f:
                            f.write(file_content)
                        self.log_msg_green(f"Downloaded file: {local_path}")
                        downloaded_file_ids.add(file_id)
                    except Exception as e:
                        print(f"Error downloading file {file_id}: {e}")
            
        except Exception as e:
            print(f"Error handling file downloads: {e}")

    def compare_with_master(self, directory: Path) -> tuple[bool, str]:
        """
        Compare files against the master roles document and identify faults.
        
        Args:
            directory: Path to the directory containing the files to compare
            
        Returns:
            tuple[bool, str]: (has_fault, error_message)
        """
        try:
            # Find the master roles document (case-insensitive)
            master_file = None
            for file_path in directory.glob("*"):
                if file_path.name.lower().startswith("roles"):
                    master_file = file_path
                    break
            
            if not master_file:
                return True, "Master roles document not found in directory"
                
            # Get master document content
            import pandas as pd
            try:
                master_df = pd.read_excel(str(master_file))
                # Convert to string representation for comparison
                master_content = master_df.to_string()
            except Exception as e:
                return True, f"Error reading master document: {str(e)}"
                
            # Compare other documents
            faults = []
            for file_path in directory.glob("*.xlsx"):  # Only process Excel files
                if file_path == master_file:
                    continue
                    
                try:
                    current_df = pd.read_excel(str(file_path))
                    current_content = current_df.to_string()
                    
                    # Focus on User ID and System differences
                    if 'User ID' in master_df.columns and 'System' in master_df.columns:
                        # Get user IDs that exist in both files
                        if 'User ID' in current_df.columns and 'System' in current_df.columns:
                            common_user_ids = set(master_df['User ID']) & set(current_df['User ID'])
                            
                            for user_id in common_user_ids:
                                master_system = master_df[master_df['User ID'] == user_id]['System'].iloc[0]
                                current_system = current_df[current_df['User ID'] == user_id]['System'].iloc[0]
                                
                                # Report if the system values differ
                                if pd.notna(current_system) and pd.notna(master_system) and master_system != current_system:
                                    # Get the user's name and role from master document
                                    user_name = master_df[master_df['User ID'] == user_id]['Name'].iloc[0] if 'Name' in master_df.columns else 'Unknown Name'
                                    master_role = master_df[master_df['User ID'] == user_id]['Role'].iloc[0] if 'Role' in master_df.columns else 'Unknown Role'
                                    fault_msg = f"User ID {user_id} - {user_name} (Role: {master_role}) has different system values:\n  - In master: {master_system}\n  - In {file_path.name}: {current_system}"
                                    faults.append(fault_msg)
                        
                except Exception as e:
                    faults.append(f"Error processing {file_path.name}: {str(e)}")
            
            # Always create error summary file
            error_summary = "\n".join([
                "System Value Discrepancy Report",
                "============================",
                f"Master Document: {master_file.name}",
                f"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}",
                "",
                "System Check Results:",
                "---------------------------",
                "Status: " + ("FAULTS DETECTED" if faults else "NO FAULTS"),
                "",
                "Details:",
                "---------------------------"
            ] + ([f"{fault}" for fault in faults] if faults else ["All system values match master document"]))
            
            # Ensure the directory exists
            directory.mkdir(parents=True, exist_ok=True)
            
            error_file = directory / "error.txt"
            
            try:
                # Always write the new error report
                error_file.write_text(error_summary, encoding='utf-8')
                self.log_msg_purple(f"Error report {'updated with discrepancies' if faults else 'created (no faults)'}")
                
                # Clean up Excel files after successful error report update
                for xlsx_file in directory.glob("*.xlsx"):
                    try:
                        xlsx_file.unlink()
                        self.log_msg_purple(f"Cleaned up: {xlsx_file.name}")
                    except Exception as e:
                        self.log_msg_purple(f"Error removing {xlsx_file.name}: {e}")
                
                # Always upload error.txt to blob storage
                if self.upload_blob(error_file, "error"):
                    self.log_msg_green("Successfully uploaded error.txt to blob storage")
                else:
                    self.log_msg_purple("Failed to upload error.txt to blob storage")
                    
            except Exception as e:
                self.log_msg_purple(f"Error writing error.txt: {str(e)}")
                return True, f"Error writing error.txt: {str(e)}"
            
            return bool(faults), f"{'Found ' + str(len(faults)) + ' faults' if faults else 'No faults detected'}. Details written to error.txt"
            
        except Exception as e:
            return True, f"Error during comparison: {str(e)}"

    def search_local_files(self, directory: Path, search_term: str) -> list[Path]:
        """Search for files in the local directory using a search term."""
        self.log_msg_purple(f"Searching in {directory} for: {search_term}")
        
        try:
            # Convert search term to lowercase for case-insensitive search
            search_term = search_term.lower()
            results = []
            
            # Make sure directory exists
            if not directory.exists():
                self.log_msg_purple(f"Directory {directory} does not exist")
                return results
                
            # Search through all PDF files in the directory
            for file_path in directory.glob("**/*.pdf"):
                try:
                    # Check filename first
                    if search_term in file_path.name.lower():
                        results.append(file_path)
                        continue
                        
                    # If not found in filename, check content
                    from pdfminer.high_level import extract_text
                    content = extract_text(str(file_path)).lower()
                    
                    if search_term in content:
                        results.append(file_path)
                        self.log_msg_green(f"Found match in: {file_path.name}")
                        
                except Exception as file_error:
                    self.log_msg_purple(f"Error processing file {file_path}: {str(file_error)}")
                    continue
            
            self.log_msg_green(f"Found {len(results)} matching files")
            return results
            
        except Exception as e:
            self.log_msg_purple(f"Error during search: {str(e)}")
            return []

    def download_from_blob_storage(self, storage_account_name: str, sas_token: str, container_name: str, target_dir: Path) -> list[Path]:
        """Download new or updated files from Azure Blob Storage using SAS token."""
        self.log_msg_purple(f"=== Starting Azure Blob Storage Synchronization ===")
        
        # Validate inputs
        if not storage_account_name or not storage_account_name.strip():
            raise ValueError("Storage account name cannot be empty")
        if not sas_token or not sas_token.strip():
            raise ValueError("SAS token cannot be empty")
        if not container_name or not container_name.strip():
            raise ValueError("Container name cannot be empty")
            
        # Log configuration
        self.log_msg_purple(f"Storage Account: {storage_account_name}")
        self.log_msg_purple(f"Container: {container_name}")
        self.log_msg_purple(f"Target Directory: {target_dir}")
        downloaded_files = []
        
        try:
            # Construct the account URL with SAS token
            container_url = f"https://{storage_account_name}.blob.core.windows.net/{container_name}?{sas_token}"
            self.log_msg_purple(f"Establishing connection to Azure Storage container...")
            
            try:
                # Create the container client using container URL with SAS
                self.log_msg_purple("Initializing Container Client...")
                container_client = ContainerClient.from_container_url(container_url)
                
                # Test connection
                self.log_msg_purple("Testing connection...")
                list(container_client.list_blobs())[0:1]  # Try to list first blob
                self.log_msg_green("âœ“ Successfully connected to container")
                
            except Exception as auth_error:
                error_message = str(auth_error)
                if "AuthorizationFailure" in error_message:
                    self.log_msg_purple("âŒ Authorization Failed. Common causes:")
                    self.log_msg_purple("   1. Invalid SAS token")
                    self.log_msg_purple("   2. Token does not have sufficient permissions")
                    self.log_msg_purple("   3. Token has expired")
                raise Exception(f"Authentication failed: {error_message}")
            
            self.log_msg_purple(f"Accessing container '{container_name}'...")
            
            # Ensure target directory exists
            target_dir.mkdir(parents=True, exist_ok=True)
            
            # Get list of existing local files and their sizes
            self.log_msg_purple("Scanning local directory for existing files...")
            local_files = {}
            total_local_size = 0
            for file_path in target_dir.glob('*'):
                if file_path.is_file():
                    file_size = file_path.stat().st_size
                    local_files[file_path.name] = file_size
                    total_local_size += file_size
            
            self.log_msg_purple(f"Found {len(local_files)} existing local files")
            self.log_msg_purple(f"Total local storage used: {total_local_size / (1024*1024):.2f} MB")
            
            # List all blobs in the container
            self.log_msg_purple("Retrieving blob list from Azure Storage...")
            blob_list = list(container_client.list_blobs())
            total_blob_size = sum(blob.size for blob in blob_list)
            self.log_msg_purple(f"Found {len(blob_list)} files in blob storage")
            self.log_msg_purple(f"Total blob storage size: {total_blob_size / (1024*1024):.2f} MB")
            
            # Compare and download only new or updated files
            for blob in blob_list:
                try:
                    target_path = target_dir / blob.name
                    should_download = False
                    
                    # Check file status
                    if blob.name not in local_files:
                        self.log_msg_purple(f"ðŸ“„ New file detected: {blob.name}")
                        self.log_msg_purple(f"   Size: {blob.size / 1024:.2f} KB")
                        should_download = True
                    elif local_files[blob.name] != blob.size:
                        self.log_msg_purple(f"ðŸ”„ File update detected: {blob.name}")
                        self.log_msg_purple(f"   Current size: {local_files[blob.name] / 1024:.2f} KB")
                        self.log_msg_purple(f"   New size: {blob.size / 1024:.2f} KB")
                        should_download = True
                    
                    if should_download:
                        self.log_msg_purple(f"â¬‡ï¸  Downloading: {blob.name}")
                        self.log_msg_purple(f"   Destination: {target_path}")
                        self.log_msg_purple(f"   Size: {blob.size / 1024:.2f} KB")
                        
                        # Track download progress
                        start_time = time.time()
                        blob_client = container_client.get_blob_client(blob.name)
                        
                        with open(target_path, "wb") as file:
                            data = blob_client.download_blob()
                            file.write(data.readall())
                            
                        end_time = time.time()
                        duration = end_time - start_time
                        speed = (blob.size / 1024 / 1024) / duration  # MB/s
                        
                        downloaded_files.append(target_path)
                        self.log_msg_green(f"âœ… Successfully downloaded: {target_path}")
                        self.log_msg_green(f"   Duration: {duration:.2f} seconds")
                        self.log_msg_green(f"   Speed: {speed:.2f} MB/s")
                    else:
                        self.log_msg_purple(f"â­ï¸  Skipping unchanged file: {blob.name}")
                        self.log_msg_purple(f"   Size: {blob.size / 1024:.2f} KB")
                    
                except Exception as blob_error:
                    self.log_msg_purple(f"Error processing blob {blob.name}: {str(blob_error)}")
                    continue
            
            self.log_msg_green(f"Downloaded {len(downloaded_files)} new or updated files")
            return downloaded_files
            
        except Exception as e:
            self.log_msg_purple(f"Error accessing blob storage: {str(e)}")
            return downloaded_files
            
    def upload_blob(self, file_path: Path, blob_name: str) -> bool:
        """Upload a file to blob storage with the specified name."""
        try:
            # Get connection string from environment
            connection_string = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
            if not connection_string:
                self.log_msg_purple("Missing storage connection string")
                return False

            # Create the BlobServiceClient
            blob_service_client = BlobServiceClient.from_connection_string(connection_string)
            
            # Get container name - use the same container as download
            container_name = os.getenv("AZURE_CONTAINER_NAME", "datasheets")
            
            try:
                # First try to get the container
                container_client = blob_service_client.get_container_client(container_name)
                try:
                    # Check if container exists in a separate try block
                    exists = container_client.exists()
                    if not exists:
                        self.log_msg_purple(f"Container {container_name} does not exist, creating it...")
                        try:
                            # Create container in a separate try block
                            container_client = blob_service_client.create_container(container_name)
                            self.log_msg_green(f"Created container {container_name}")
                        except Exception as create_error:
                            self.log_msg_purple(f"Error creating container: {str(create_error)}")
                            # Check if it's a permission error
                            if "AuthorizationFailure" in str(create_error):
                                self.log_msg_purple("Please check your storage account permissions")
                            return False
                except Exception as exists_error:
                    self.log_msg_purple(f"Error checking if container exists: {str(exists_error)}")
                    return False
            except Exception as container_error:
                self.log_msg_purple(f"Error accessing container: {str(container_error)}")
                # Check common error conditions
                if "InvalidResourceName" in str(container_error):
                    self.log_msg_purple(f"Invalid container name: {container_name}. Container names must be lowercase letters, numbers, and hyphens.")
                elif "AuthenticationFailed" in str(container_error):
                    self.log_msg_purple("Authentication failed. Please check your storage connection string.")
                return False
            
            # Upload the file with detailed progress information
            try:
                # Check file exists and get size
                try:
                    file_size = file_path.stat().st_size
                except Exception as file_error:
                    self.log_msg_purple(f"Error accessing file {file_path}: {str(file_error)}")
                    return False

                self.log_msg_purple(f"\nPreparing to upload {file_path.name}:")
                self.log_msg_purple(f"  â€¢ File size: {file_size / 1024:.2f} KB")
                self.log_msg_purple(f"  â€¢ Destination: {container_name}/{blob_name}")
                self.log_msg_purple(f"  â€¢ Overwrite if exists: Yes")
                
                start_time = time.time()
                try:
                    blob_client = container_client.get_blob_client(blob_name)
                    
                    try:
                        with open(file_path, "rb") as data:
                            self.log_msg_purple("Starting upload...")
                            blob_client.upload_blob(data, overwrite=True)
                    except Exception as upload_error:
                        self.log_msg_purple(f"Error during file upload: {str(upload_error)}")
                        if "MD5 validation failed" in str(upload_error):
                            self.log_msg_purple("Upload failed validation check - possible corruption during transfer")
                        elif "timeout" in str(upload_error).lower():
                            self.log_msg_purple("Upload timed out - please check your network connection")
                        return False
                        
                    end_time = time.time()
                    duration = end_time - start_time
                    speed = (file_size / 1024 / 1024) / duration if duration > 0 else 0  # MB/s
                    
                    self.log_msg_green("\nUpload completed successfully:")
                    self.log_msg_green(f"  â€¢ Duration: {duration:.2f} seconds")
                    self.log_msg_green(f"  â€¢ Speed: {speed:.2f} MB/s")
                    self.log_msg_green(f"  â€¢ Blob URL: {blob_client.url}")
                    
                except Exception as blob_error:
                    self.log_msg_purple(f"Error creating blob client: {str(blob_error)}")
                    return False
                    
            except Exception as e:
                self.log_msg_purple(f"Unexpected error during upload: {str(e)}")
                return False
                
            self.log_msg_green(f"Successfully uploaded {file_path} to blob {blob_name}")
            return True
            
        except Exception as e:
            self.log_msg_purple(f"Error uploading blob: {str(e)}")
            return False
            
    def sync_vector_store_files(self, project_client: AIProjectClient, vector_store_id: str, target_dir: Path) -> list[Path]:
        """Download and sync all files from a vector store to a local directory."""
        self.log_msg_purple(f"Starting sync for vector store: {vector_store_id}")
        self.log_msg_purple(f"Target directory: {target_dir.absolute()}")
        
        downloaded_files = []
        
        try:
            # Ensure target directory exists
            target_dir.mkdir(parents=True, exist_ok=True)
            self.log_msg_purple("Target directory created/verified")
            
            # Get vector store details
            self.log_msg_purple("Fetching vector store details...")
            vector_store = project_client.agents.get_vector_store(vector_store_id)
            self.log_msg_green(f"Found vector store with {len(vector_store.file_ids)} files")
            
            # Process each file
            for file_id in vector_store.file_ids:
                try:
                    self.log_msg_purple(f"Processing file ID: {file_id}")
                    
                    # Get file metadata using agents API
                    file_info = project_client.agents.get_file(file_id)
                    target_path = target_dir / f"{file_info.filename}"
                    
                    self.log_msg_purple(f"Downloading to: {target_path.absolute()}")
                    
                    # Download file content
                    with target_path.open("wb") as f:
                        for chunk in project_client.agents.download_file(file_id):
                            f.write(chunk)
                    
                    downloaded_files.append(target_path)
                    self.log_msg_green(f"Successfully downloaded: {target_path.name}")
                    
                except Exception as file_error:
                    self.log_msg_purple(f"Error downloading file {file_id}: {str(file_error)}")
                    continue
                    
            return downloaded_files
            
        except Exception as e:
            self.log_msg_purple(f"Error in vector store sync: {str(e)}")
            return downloaded_files
        for file_id in vector_store.file_ids:
            try:
                # Get file metadata
                file_info = project_client.files.get(file_id)
                target_path = target_dir / f"{file_info.filename}"
                
                self.log_msg_purple(f"Downloading: {file_info.filename}")
                
                # Download and write file
                with target_path.open("wb") as f:
                    for chunk in project_client.files.download(file_id):
                        f.write(chunk)
                        
                downloaded_files.append(target_path)
                self.log_msg_green(f"Successfully downloaded: {target_path}")
                
            except Exception as e:
                self.log_msg_purple(f"Error downloading file {file_id}: {e}")
                continue
                
        return downloaded_files
